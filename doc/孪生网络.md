# 孪生神经网络
普通神经网络需要大量数据进行训练，对于数据有限场景，需考虑其它方法。
孪生神经网络Siamese Network，孪生即存在连体，连体即彼此共享一部分，孪生神经网络一般包括两个相同结构的子网络，子网络之间共享权重,子网络不共享权重时,通常定义为伪孪生神经网络.

## 优势
1. 对于类别不平衡问题更具鲁棒性,可以从语义相似性上学习来估测两个输入的距离.
2. 孪生神经网络输出的是俩哥哥类间的距离而不是概率.

## 损失函数
孪生神经网络是计算两个输入的相似度,距离,而不是对输入做分类,因此交叉熵损失函数不适用,孪生神经网络常用的损失函数有Triplet Loss和Contrastive Loss
### Triplet Loss
三元组损失函数,起源于谷歌2015的facenet,该损失函数定义一个三元组作为输入,分别是(X_anchor, X_positive, X_negative),anchor是训练集中的随机样本,positive是同一类的样本,和一个不同类别样本的negative.以此构建三元组,将其输入到网络可以得到对应的特征向量[f(X_anchor),f(X_positive),f(X_negative)],Triplet Loss的目的就是通过训练是的同类别的距离更近,不同类别的距离更远.

总的来说,Triplet适用于度量学习,是基于相似度计算,基本理念是减小同类样本间的loss,增大不同类别间的loss.
```
L(X_anchor, X_positive, X_negative)=max(||f(X_anchor)−f(X_positive)||−||f(X_anchor)−f(X_negative)||+α,0)
```
以上是loss函数的公式化表示,其中a是一个设定的常量,表示不同类间的目标区分距离.

### Constastive Loss
这是另外一种用于衡量相似度的常用函数,由Yann Lecun在2005年提出.

Constastive Loss的输入是一对样本,基于相似的一对样本特征距离应该更小,不同类也就是相似度第的一对对象特征距离应该尽可能大.
构建一对样本,从数据集从选取一对样本[X_a, X_b],距离计算选择欧式距离d=||X_a - X_b||_2 = (X_a - X_b)^2
```
L(X_a, X_b) = ((1 - Y)*d^2)/2 + (Y {max(0, m - d)}^2)/2
```
其中Y表示两个样本是否匹配,匹配为1,不匹配为0,m是安全距离,当两个样本距离小于m时,Loss函数将变为0,这使得两样本相似而不是相同,保留网络的泛化能力.
## 动手实现一个孪生网络
https://gitee.com/lwgaoxin/Siamese-pytorch/
https://gitee.com/lx_r/object_detection_task/tree/main/siamese
需要重构dataloader和samplegenerater,构建对样本和重构net的forward函数,重点在于sample和lable,lable还没搞清楚怎么构建的.
