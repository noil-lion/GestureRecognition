# 注意力机制-在时序数据挖掘上
时间序列数据具有时序相关性，并且时序特征在时间维度上具有不同的重要程度，某些子序列的特征重要程度远大于其它子序列，不同特征具有被不同的重要程度。例如在特定的运动时，不同的运动场景会有




# self-attention
```
import math
import torch
import torch.nn as nn


class SelfAttention(nn.Module):
    def __init__(self, input_dim, dim_q, dim_v):
        super(SelfAttention, self).__init__()

        # dim_q = dim_k
        self.dim_q, self.dim_k, self.dim_v = dim_q, dim_q, dim_v

        self.Q = nn.Linear(input_dim, dim_q)
        self.K = nn.Linear(input_dim, dim_q)
        self.V = nn.Linear(input_dim, dim_v)

        self.softmax = nn.Softmax(dim=-1)

    def forward(self, x):
        # Q: [batch_size,seq_len,dim_q]
        # K: [batch_size,seq_len,dim_k]
        # V: [batch_size,seq_len,dim_v]
        Q, K, V = self.Q(x), self.K(x), self.V(x)
        print(f'x.shape:{x.shape} , Q.shape:{Q.shape} , K.shape: {K.shape} , V.shape:{V.shape}')
        attention = torch.bmm(self.softmax(torch.bmm(Q, K.permute(0, 2, 1)) / math.sqrt(self.dim_k)), V)
        return attention


if __name__ == '__main__':
    batch_size = 2  # 批量数
    input_dim = 5  # 句子中每个单词的向量维度
    seq_len = 3  # 句子长度
    x = torch.randn(batch_size, seq_len, input_dim)
    self_attention = SelfAttention(input_dim, batch_size, batch_size + input_dim)
    print(x)
    print('=' * 50)
    attention = self_attention(x)
    print('=' * 50)
    print(attention)
```
```
def plot_attention_weights(attention, sentence, result, layer): fig=plt. figure(figsize=(16,8))
sentence=tokenizer. encode(sentence)
attention=tf. squeeze(attention[ layer], axis=0)
for head in range(attention. shape[0]): ax=fig. add_subplot(2,4, head+1)
# plot the attention weights ax. matshow(attention[ head][:-1,:], cmap=' viridis')
fontdict={' fontsize':18}
ax. set_xticks(range(1en(sentence)+2))
ax. set_yticks(range(1en(result))) ax. set_y1im(1en(result)-1.5,-0.5)
ax. set_xticklabels(
['<start>"]+[ tokenizer. decode([i]) for i in sentence]+['<end>], fontdict=fontdict, rotation=9e)
ax. set yticklabels([ tokenizer. decode([i]) for i in result
ifi<tokenizer. vocab_size], fontdict=fontdict)
ax. set_xlabel(' Head {}'. format(head+1))
plt. tight_layout()
plt. show()
```